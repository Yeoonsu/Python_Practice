{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
      "     -------------------------------------- 128.2/128.2 kB 2.5 MB/s eta 0:00:00\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.11.1 soupsieve-2.3.2.post1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, usecsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib.request as ur\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "url = 'http://quotes.toscrape.com/'\n",
    "\n",
    "\n",
    "#urllib.request.urlopen('URL 주소')\n",
    "html = ur.urlope(url)\n",
    "\n",
    "html.read()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파싱하기 쉬운 형태로 바꾸기\n",
    "#bs(html.read(), 'html.parser')\n",
    "\n",
    "html = ur.urlopen(url)\n",
    "soup = bs(html.read(), 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 과정을 한줄로 표현하기\n",
    "\n",
    "#soup = bs(ur.urlopen(URL 주소).read(), 'html.parser')\n",
    "soupt = bs(ur.urlopen('http://quotes.toscrape.com/').read(), 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote = soup.find_all('span')\n",
    "quote[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in quote:\n",
    "    i.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.find_all('div', {\"class\":\"quote\"})[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in soup.find_all('div', {\"class\":\"quote\"}):\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06-3 포털 사이트에서 기사 크롤링하기\n",
    "import os, re\n",
    "import urllib.request as ur\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "os.chdir(r'C:\\Users\\user\\python') # 파일 저장 경로 앞에 r 붙이는거 기억하기\n",
    "\n",
    "news = 'https://news.daum.net/'\n",
    "soup = bs(ur.urlopen(news).read(), 'html.parser')\n",
    "\n",
    "# 확실한 코드 하나를 먼저 만들고 그 다음에 반복문을 적용하기\n",
    "\n",
    "soup.find_all('div', {\"class\": \"item_issue\"})\n",
    "\n",
    "soup.find_all('div', {\"class\":\"item_issue\"})\n",
    "\n",
    "for i in soup.find_all('div', {\"class\":\"item_issue\"}):\n",
    "    i.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼링크 주소 추출하기\n",
    "soup.find_all('a')[:5]\n",
    "\n",
    "# get으로 <a> 태그에서 href 속성값을 출력해보기\n",
    "for i in soup.find_all('a')[:5]:\n",
    "    i.get('href')\n",
    "\n",
    "news = 'https://news.daum.net/'\n",
    "soup = bs(ur.urlopen(news).read(), 'html.parser')\n",
    "\n",
    "for i in soup.find_all('div', {\"class\":\"item_issue\"}):\n",
    "    i.find_all('a')[0].get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기사 제목과 내용 한꺼번에 추출하기\n",
    "\n",
    "article1 = 'https://go.seoul.co.kr/news/newsView.......'\n",
    "\n",
    "soup2 = bs(ur.urlopen(article1).read(), 'html.parser')\n",
    "\n",
    "for i in soup2.find_all('p'):\n",
    "    print(i.text)\n",
    "\n",
    "headline = soup.find_all('div', {\"class\" : \"item_issue\"})\n",
    "\n",
    "print(headline[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼링크된 모든 기사의 제목과 본문 추출하기\n",
    "\n",
    "for i in headline:\n",
    "    print(i.text, '\\n')\n",
    "    soup3 = bs(ur.urlopen(i.find_all('a')[0].get('href')).read(), 'html.parser')\n",
    "    \n",
    "    for j in soup3.find_all('p'):\n",
    "        j.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웹 크롤링 실행파일 만들기\n",
    "\n",
    "pip install pyinstaller\n",
    "\n",
    "# article_collector.py\n",
    "\n",
    "from lib2to3.pygram import python_grammar_no_print_statement\n",
    "import os, codecs, re, datetime, requests\n",
    "import urllib.request as ur\n",
    "from bs4 import BeautifulSoup as bs\n",
    "os.chdir(r'C:\\Users\\user\\python')\n",
    "url = 'https://news.daum.net/'\n",
    "f = open(str(datetime.date.today()) + 'articles.txt', 'w')\n",
    "soup = bs(ur.urlopen(url).read(), 'html.parser')\n",
    "\n",
    "for i in soup.find_all('div',{\"class\":\"thumb_relate\"}):\n",
    "    try:\n",
    "        f.write(i.text+'\\n')\n",
    "        f.write(i.find_all('a')[0].get('href') + '\\n')\n",
    "        soup2 = bs(ur.urlopen(i.find_all('a')[0].get('href')).read(), 'html.parser')\n",
    "        for j in soup2.find_all('p'):\n",
    "            f.write(j.text)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "f.close()\n",
    "\n",
    "# 파일이 저장된 경로로 이동한 뒤 다음과 같이 명령 프롬프트에서 명령\n",
    "pyinstaller --onefile [파이썬 파일].py\n",
    "\n",
    "pyinstaller --onefile article_collector.py\n",
    "\n",
    "# 폴더에 가보면 article_collector.exe가 생성되어 있음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "757aa4992fb8ed25d3a96931c0ea3cbac88aa5e1f2d55e5c1a4c08013c8200eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
